{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Namrahh/AI-Projects/blob/main/LLM_Assignment_Search_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQMaHq0wyqFZ"
      },
      "source": [
        "# **1. Setting up the google colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "6RLSkcndyVO_",
        "outputId": "402e0ece-4aa7-41cc-b9de-db9d93861bfb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-56d40476-dec3-4498-a969-bfe2b4608cf9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-56d40476-dec3-4498-a969-bfe2b4608cf9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving fcomm-09-1465178.pdf to fcomm-09-1465178.pdf\n",
            "Saving fcomm-09-1456509.pdf to fcomm-09-1456509.pdf\n",
            "Saving media__entertainment_5086_nieuw.aa6e53fd64e3.pdf to media__entertainment_5086_nieuw.aa6e53fd64e3 (2).pdf\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload your PDF file\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbmhPubLzChg"
      },
      "source": [
        "# **2. Extract text from PDF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZsMGo2TzKTX",
        "outputId": "0802412e-45ba-4c3c-b418-49371a9de7d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "# List of PDF paths (the keys of the uploaded files)\n",
        "pdf_paths = list(uploaded.keys())\n",
        "\n",
        "# Extract text from each PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, \"rb\") as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Dictionary to store extracted text from each PDF\n",
        "pdf_texts = {}\n",
        "\n",
        "# Loop through each uploaded PDF file\n",
        "for pdf_path in pdf_paths:\n",
        "    pdf_texts[pdf_path] = extract_text_from_pdf(pdf_path)\n",
        "    print(f\"Extracted Text from {pdf_path} (sample):\", pdf_texts[pdf_path][:500])  # Print a small sample of the extracted text\n",
        "\n",
        "# Optionally, if you want to combine the text from all PDFs into one large corpus\n",
        "combined_text = \"\\n\".join(pdf_texts.values())\n",
        "\n",
        "# Now you can continue with your further processing using `pdf_texts` or `combined_text`\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5M4c89HKU8g",
        "outputId": "b53197ec-9d90-456b-c3e0-50ace80de228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Text from fcomm-09-1465178.pdf (sample): Frontiers in Communication 01 frontiersin.org\n",
            "Ethics and journalistic challenges \n",
            "in the age of artificial intelligence: \n",
            "talking with professionals and \n",
            "experts\n",
            "Beatriz Gutiérrez-Caneda 1*, Carl-Gustav Lindén 2 and \n",
            "Jorge Vázquez-Herrero 1\n",
            "1 Departamento de Ciencias da Comunicación, Universidade de Santiago de Compostela, Santiago de \n",
            "Compostela, Spain, 2 Department of Information Science and Media Studies, University of Bergen, \n",
            "Bergen, Norway\n",
            "The rapid advancement of artificial intelligence (\n",
            "Extracted Text from fcomm-09-1456509.pdf (sample): Frontiers in Communication 01 frontiersin.org\n",
            "The anthropomorphic pursuit of \n",
            "AI-generated journalistic texts: \n",
            "limits to expressing subjectivity\n",
            "Cristian González-Arias 1,2*, Eirini Chatzikoumi 3 and \n",
            "Xosé López-García 1\n",
            "1 Departamento de Ciencias de la Comunicación, Universidad de Santiago de Compostela, Santiago \n",
            "de Compostela, Spain, 2 Instituto de Literatura y Ciencias del Lenguaje, Pontificia Universidad Católica \n",
            "de Valparaíso, Valparaíso, Chile, 3 Facultad de Comunicaciones y Artes, Univ\n",
            "Extracted Text from media__entertainment_5086_nieuw.aa6e53fd64e3 (2).pdf (sample): The Future of\n",
            "Media & \n",
            "EntertainmentImmersive, \n",
            "interactive, \n",
            "blended world\n",
            "A view by \n",
            "Cognizant’s Center \n",
            "for the Future of \n",
            "Work \n",
            "10\n",
            "4The future of media & entertainment\n",
            "will offer a world of opportunities, \n",
            "but the transformation will require \n",
            "substantial steps and boldness. The \n",
            "industry needs to be open to new \n",
            "insights instead of being mentally  \n",
            "closed by existing ones. \n",
            "Each industry has its owns challenges, and digital disruption is everywhere. \n",
            "Organizations must be agile and build new\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UdHgOSmzW66"
      },
      "source": [
        "# **3. Split the text into smaller documents**"
      ]
    },
    {
      "source": [
        "# Generate chunks from the PDF text\n",
        "# Concatenate all text into a single string before splitting\n",
        "all_text = \" \".join(pdf_texts.values())\n",
        "\n",
        "def split_text_into_chunks(text, chunk_size=1000):\n",
        "    \"\"\"Splits the text into chunks of a specified size.\n",
        "\n",
        "    Args:\n",
        "        text: The text to split.\n",
        "        chunk_size: The desired size of each chunk. Defaults to 1000.\n",
        "\n",
        "    Returns:\n",
        "        A list of text chunks.\n",
        "    \"\"\"\n",
        "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "chunks = list(split_text_into_chunks(all_text))\n",
        "print(\"Number of Chunks:\", len(chunks))\n",
        "print(\"Sample Chunk:\", chunks[0])"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVg6KxnwLT06",
        "outputId": "2099fbca-5b09-457c-9e75-ccc2ca6b8b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Chunks: 164\n",
            "Sample Chunk: Frontiers in Communication 01 frontiersin.org\n",
            "Ethics and journalistic challenges \n",
            "in the age of artificial intelligence: \n",
            "talking with professionals and \n",
            "experts\n",
            "Beatriz Gutiérrez-Caneda 1*, Carl-Gustav Lindén 2 and \n",
            "Jorge Vázquez-Herrero 1\n",
            "1 Departamento de Ciencias da Comunicación, Universidade de Santiago de Compostela, Santiago de \n",
            "Compostela, Spain, 2 Department of Information Science and Media Studies, University of Bergen, \n",
            "Bergen, Norway\n",
            "The rapid advancement of artificial intelligence (AI) is transforming the media \n",
            "industry by automating processes, with applications in data analysis, automated \n",
            "writing, format transformation, content personalization, and fact-checking. \n",
            "While AI integration offers new opportunities in journalism, it also raises ethical \n",
            "concerns around data privacy, algorithmic biases, transparency, and potential job \n",
            "displacement. This study employed qualitative interviews with media professionals \n",
            "and researchers to explore their perspectives on the ethical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6tFQKiJzmgN"
      },
      "source": [
        "# **4. Preprocess text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8yi5SiGzqTc",
        "outputId": "562fce4d-64e5-4952-ce50-ba291a96e107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed Corpus Sample: [['frontiers', 'in', 'communication', 'frontiersin', 'org', 'ethics', 'and', 'journalistic', 'challenges', 'in', 'the', 'age', 'of', 'artificial', 'intelligence', 'talking', 'with', 'professionals', 'and', 'experts', 'beatriz', 'gutiérrez', 'caneda', 'carl', 'gustav', 'lindén', 'and', 'jorge', 'vázquez', 'herrero', 'departamento', 'de', 'ciencias', 'da', 'comunicación', 'universidade', 'de', 'santiago', 'de', 'compostela', 'santiago', 'de', 'compostela', 'spain', 'department', 'of', 'information', 'science', 'and', 'media', 'studies', 'university', 'of', 'bergen', 'bergen', 'norway', 'the', 'rapid', 'advancement', 'of', 'artificial', 'intelligence', 'ai', 'is', 'transforming', 'the', 'media', 'industry', 'by', 'automating', 'processes', 'with', 'applications', 'in', 'data', 'analysis', 'automated', 'writing', 'format', 'transformation', 'content', 'personalization', 'and', 'fact', 'checking', 'while', 'ai', 'integration', 'offers', 'new', 'opportunities', 'in', 'journalism', 'it', 'also', 'raises', 'ethical', 'concerns', 'around', 'data', 'privacy', 'algorithmic', 'biases', 'transparency', 'and', 'potential', 'job', 'displacement', 'this', 'study', 'employed', 'qualitative', 'interviews', 'with', 'media', 'professionals', 'and', 'researchers', 'to', 'explore', 'their', 'perspectives', 'on', 'the', 'ethical', 'implications', 'of', 'ai', 'integration', 'in', 'newsrooms', 'interview', 'data', 'were', 'analyzed', 'to', 'identify', 'common', 'themes', 'and', 'specific', 'challenges', 'related', 'to', 'ai', 'use', 'in', 'journalism', 'the', 'findings', 'discuss', 'issues', 'such', 'as', 'the', 'tensions', 'between', 'technology', 'and', 'journalism', 'ethical', 'challenges', 'related', 'to', 'ai', 'the', 'evolution', 'of', 'professional', 'roles', 'in', 'journalism', 'media', 'guidelines', 'and', 'potential', 'future', 'regulations', 'keywords', 'ai', 'ai', 'journalism', 'ethics', 'ai', 'guidelines', 'algorithmic', 'journalism', 'introduction', 'after', 'more', 'than', 'three', 'decades', 'of', 'digital', 'journalism', 'salaverría', 'news', 'is', 'going', 'through', 'period', 'of', 'immense', 'change', 'and', 'challenges', 'the', 'communication', 'paradigm', 'has', 'shifted', 'from', 'unidirectional', 'model', 'to', 'bidirectional', 'model', 'transforming', 'the', 'dynamics', 'of', 'media', 'outlets', 'audience', 'consumption', 'and', 'consequently', 'business', 'models', 'legacy', 'media', 'once', 'the', 'gatekeepers', 'of', 'information', 'currently', 'fight', 'with', 'platforms', 'and', 'opinion', 'leaders', 'in', 'an', 'unequal', 'battle', 'trying', 'to', 'keep', 'their', 'audiences', 'and', 'conquer', 'new', 'ones', 'while', 'fighting', 'trust', 'crisis', 'that', 'is', 'worsening', 'due', 'to', 'growth', 'in', 'dis', 'and', 'misinformation', 'under', 'these', 'circumstances', 'newsrooms', 'are', 'exploring', 'new', 'formats', 'and', 'approaching', 'new', 'social', 'media', 'platforms', 'such', 'as', 'tiktok', 'vázquez', 'herrero', 'et'], ['al', 'technological', 'innovation', 'is', 'also', 'underway', 'with', 'the', 'integration', 'of', 'high', 'technology', 'solutions', 'such', 'as', 'virtual', 'reality', 'vr', 'or', 'artificial', 'intelligence', 'ai', 'lópez', 'garcía', 'and', 'vizoso', 'pérez', 'seijo', 'et', 'al', 'regarding', 'technological', 'innovation', 'media', 'organizations', 'sometimes', 'focus', 'on', 'bright', 'shiny', 'things', 'getting', 'carried', 'away', 'by', 'the', 'hype', 'of', 'newness', 'consequently', 'some', 'voices', 'within', 'the', 'journalism', 'environment', 'advocate', 'for', 'more', 'critical', 'reflective', 'practice', 'and', 'research', 'informed', 'approaches', 'posetti', 'in', 'this', 'scenario', 'ai', 'has', 'emerged', 'as', 'set', 'of', 'disruptive', 'technologies', 'including', 'machine', 'learning', 'such', 'as', 'neural', 'networks', 'and', 'deep', 'learning', 'as', 'well', 'as', 'natural', 'language', 'processing', 'nlp', 'techniques', 'such', 'as', 'speech', 'and', 'text', 'recognition', 'analysis', 'and', 'generation', 'these', 'advancements', 'are', 'transforming', 'multiple', 'industries', 'including', 'journalism', 'this', 'trend', 'has', 'been', 'open', 'access', 'edited', 'by', 'simón', 'peña', 'fernández', 'university', 'of', 'the', 'basque', 'country', 'spain', 'reviewed', 'by', 'javier', 'díaz', 'noci', 'pompeu', 'fabra', 'university', 'spain', 'ana', 'serrano', 'tellería', 'university', 'of', 'castilla', 'la', 'mancha', 'spain', 'correspondence', 'beatriz', 'gutiérrez', 'caneda', 'beatriz', 'gutierrez', 'caneda', 'usc', 'es', 'received', 'july', 'accepted', 'october', 'published', 'november', 'citation', 'gutiérrez', 'caneda', 'lindén', 'and', 'vázquez', 'herrero', 'ethics', 'and', 'journalistic', 'challenges', 'in', 'the', 'age', 'of', 'artificial', 'intelligence', 'talking', 'with', 'professionals', 'and', 'experts', 'front', 'commun', 'doi', 'fcomm', 'copyright', 'gutiérrez', 'caneda', 'lindén', 'and', 'vázquez', 'herrero', 'this', 'is', 'an', 'open', 'access', 'article', 'distributed', 'under', 'the', 'terms', 'of', 'the', 'creative', 'commons', 'attribution', 'license', 'cc', 'by', 'the', 'use', 'distribution', 'or', 'reproduction', 'in', 'other', 'forums', 'is', 'permitted', 'provided', 'the', 'original', 'author', 'and', 'the', 'copyright', 'owner', 'are', 'credited', 'and', 'that', 'the', 'original', 'publication', 'in', 'this', 'journal', 'is', 'cited', 'in', 'accordance', 'with', 'accepted', 'academic', 'practice', 'no', 'use', 'distribution', 'or', 'reproduction', 'is', 'permitted', 'which', 'does', 'not', 'comply', 'with', 'these', 'terms', 'type', 'original', 'research', 'published', 'november', 'doi', 'fcomm', 'gutiérrez', 'caneda', 'et', 'al', 'fcomm', 'frontiers', 'in', 'communication', 'frontiersin', 'orgknown', 'in', 'academia', 'by', 'different', 'names', 'as', 'algorithmic', 'journalism', 'computational']]\n"
          ]
        }
      ],
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Preprocess the chunks\n",
        "preprocessed_corpus = [simple_preprocess(chunk) for chunk in chunks]\n",
        "print(\"Preprocessed Corpus Sample:\", preprocessed_corpus[:2])  # Print first two processed chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cskCGG_kzwA9"
      },
      "source": [
        "# **5. Train word embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WQ3e19bzz4l",
        "outputId": "3f5dee74-66fd-4378-c306-37d9ce8e485b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec Model Trained\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=preprocessed_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
        "print(\"Word2Vec Model Trained\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFoTtvRXz4MM"
      },
      "source": [
        "# **6. Implement vector search**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMGIDU4tz8zc",
        "outputId": "5ed72f26-3a9f-4256-a10a-9829288db44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search Results:\n",
            "('society, this does not mean that people truly understand what it entails. Moreover, this also applies to journalists and newsrooms. “I think the biggest challenge (….) is to really understand what artificial intelligence is. And that means at a more technical level how it works, what it actually does and therefore also understanding what its limits are, what its dangers are and at a high level what it is” , affirmed one interviewee. This professional and researcher also notes that, in general, we do not have a proper understanding of AI. He emphasizes that we are approaching the end of the hype cycle, and we will likely face disappointment because we expect AI to accomplish things beyond its capabilities. This concern about how journalists’ understanding of AI is closely related to how the media represents this technology. Newsrooms have an important role in building social imaginaries, and due to this, they have major responsibility regarding this issue: “This is the biggest challenge, to explain it better and to explain it better, certainly to understand it first” , explains this interviewee. The main problem related to AI representation in the media is anthropomorphism, which contributes to the perception that AI is actually intelligent. This concern was brought up only by one interviewee: “I think that it is not understood that this (AI) is software, that this is statistics. (….) At the level of representation, it is not being understood that we are talking about software and not about a living entity, a person, a thing with its own will, with decision capacity, with intelligence…. No, we are not seeing it well, ” explained. The role of big tech companies is also a concern for professionals and researchers. The enterprises that provide AI solutions for media outlets play an important role when', 0.9998591)\n",
            "('and see is curated. We also have an optional “surprise mode,” in which we can purposely have experiences that aren’t informed by algorithms. The system forgets who we are and delivers us unpersonalized content. But the normal mode is predictive and analyzed – this is how we’re entertained. In the new world of media, all our senses are stimulated. We can be anywhere and experience anything through taste, smell, sight, sound and touch. Software is the new artist. Adaptive self-learning algorithms make all music, paintings, books and movies. This machine-made content is free, while content made by humans is more expensive and the new unique. Live events are still popular and can be experienced as if you are part of the event. News is tailored to our context and is fully neutral. Left or right influences are a thing of the past. Advertising no longer exists. Algorithms determine which company fits each individual’s needs best. If desired, individuals can decide to search for new services themselves, shifting from automated consumption to free consumption. But if the world of media is high-tech, is it also human-driven? What does this future mean for the relationship between companies and consumers, privacy and human values? Can technology create real progress by fundamentally changing the media industry? It’s up to us to ponder and imagine.We’ve created a world where people have to protect themselves from ever-growing amounts of content as they’re surrounded by meaningless noise and entertainment. We live in a world of accelerated addictiveness to our devices. We spend more time than ever looking at our screens continuously and exchanging personal data with unknown organizations. In most cases, we aren’t even aware of what data we share. Free digital services are everywhere and have become persuasive. Media gave organizations the tools to connect with', 0.99985456)\n",
            "('from a journalist and the tool rewrites it and publishes it, is it plagiarism? And how can it be prevented? Algorithmic authorship and accountability. Concerning copyright and plagiarism, AI authorship emerges as another challenge: who is responsible for the content created by an algorithm? Is it really the author? ( Israel and Amer, 2022 ). Human supervision seems to be the requirement here: AI-generated content cannot be published without supervision, and the supervisor (human) would be responsible for the published content. Lack of transparency. Being transparent about how a newsroom is using AI is key to keeping the public trust. It also helps to avoid misleading situations (i.e., believing an image is real when it is AI-generated). Academia has also researched whether media outlets are being transparent about how they use AI ( Cools and Koliska, 2024 ). These are the major challenges, from a newsroom perspective when it comes to integrating AI in their work routines. However, there are two related concerns that need to be addressed and that show possible future lines of research: How media is shaping AI perception. Media outlets are contributing to building AI images. The stories news is telling about these technologies play a vital role in how the audience perceives AI and it is not always as accurate as it should be, for example, humanizing AI tools and treating them as living beings. An ethical approach would be needed here to provide some guidelines that help media inform about AI without being sensationalists or alarmists, for example (Sanguinetti, 2023 ). How AI is shaping public opinion. Due to content personalization, users tend to consume always the same kind of content, falling on what is known by cognitive psychologists as “confirmation bias” (Beckett, 2019 ): they only consume content that reinforces their previous opinions.', 0.99985355)\n",
            "('SIC] affirmed one interviewee. “Laziness” or the lack of supervision by journalists is something mentioned also by other interviewees: even if journalists know they must check and verify the outputs of an AI tool, sometimes, they do not do it. This situation can provoke two problems: one, the dependence on technology, and two, mistakes can occur leading to false information getting published. Dependence on AI makes journalists rely on digital devices and systems for daily functioning and productivity, often leading to reduced self-sufficiency and increased vulnerability to technological disruptions. “ AI can hallucinate so it’s really important that journalists understand that they have to treat this source as any other source” , said one interviewee, “they have to check, they have to verify the information coming from this source” . One of the interviewees mentioned another danger related to this technology dependence: “the algorithm can be sort of an echo chamber” . 3.4 Guidelines and further regulation Interviews showed a positive view of the new AI guidelines media outlets and other organizations are creating but with different reasonings behind this attitude. For example, one of the interviewees declared not being “a big fan of charters and codes and things because I think that journalism is a very contextual situation and that the ethical problems always happen on the border of things, in the kind of grey zone” and that, despite of this, in this case, they affirmed that “it’s quite helpful because the process of drawing up guidelines around the use of AI means that people have to find out about AI” . In addition, related to guidelines, another interviewee considered they are important because they are allowing media to be transparent about how they use AI. Regulation is a complex issue, especially regarding AI and journalism. In terms of', 0.9998512)\n",
            "('there is no reason why anthropomorphic design should be the only approach. There are alternatives that may be healthier in the long run in a journalistic context. For example, instead of saying “I can help you with that, ” an impersonal phrase like “ChatGPT can help you with that” could be used to clarify the tool’s role as a utility rather than an agent or assistant. For a healthy integration of AI in journalism, it is crucial that its use is transparent and that a critical spirit is maintained ( Crépel and Cardon, 2022 ). This means not only indicating that an AI tool has been used to generate a text, but also allowing the machine to speak impersonally as an artificial entity without consciousness or experience. Similarly, it is the responsibility of the media to educate users about the general nature of AI. In cases of anthropomorphic design, it should be emphasized that the use of “I” is a linguistic convention. Reminders or warnings about the unconscious nature of AI tools such as ChatGPT could also be implemented within the tools themselves at regular intervals during interactions.5 Conclusion In this research, we sought to answer two fundamental questions: to what extent can AI incorporate features of subjectivity when imitating human language and journalistic writing, and what are the implications of replacing or impersonating human journalists in the field of communication? Our findings suggest that AI tools can simulate subjectivity on the surface of synthetic texts, demonstrating the potential to mimic human language through the use of a range of subjective markers. However, their ability to fully replicate journalistic style is limited by the specificities of different journalistic genres, which require the nuanced inclusion of specific features according to their individual purpose. It is important to note that journalistic genres', 0.99985033)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Generate document vectors by averaging word embeddings\n",
        "def compute_doc_vector(doc, model):\n",
        "    vectors = [model.wv[word] for word in doc if word in model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
        "\n",
        "# Generate vectors for all chunks\n",
        "document_vectors = [compute_doc_vector(doc, word2vec_model) for doc in preprocessed_corpus]\n",
        "\n",
        "# Search Function\n",
        "def search(query, model, document_vectors, original_chunks):\n",
        "    # Preprocess and vectorize query\n",
        "    query_vector = compute_doc_vector(simple_preprocess(query), model)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarities = cosine_similarity([query_vector], document_vectors)\n",
        "\n",
        "    # Rank results\n",
        "    ranked_indices = np.argsort(similarities[0])[::-1]\n",
        "\n",
        "    # Retrieve results\n",
        "    results = [(original_chunks[i], similarities[0][i]) for i in ranked_indices[:5]]  # Top 5 results\n",
        "    return results\n",
        "\n",
        "# Test the search\n",
        "query = \"What is deep learning?\"\n",
        "results = search(query, word2vec_model, document_vectors, chunks)\n",
        "print(\"Search Results:\")\n",
        "for result in results:\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeVZdzA40Fpw"
      },
      "source": [
        "# **7. Fine-tune a transfer model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aOVHwPqD0Ull",
        "outputId": "609f8818-50d2-43c9-a2b1-d052b334914b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EftM4k_1Fyp9",
        "outputId": "8231ed09-5722-4d4c-ec66-4c0aa50b4425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have your dataset in `custom_dataset`\n",
        "# Define custom_dataset first\n",
        "custom_dataset = Dataset.from_dict({\"text\": chunks, \"label\": [0] * len(chunks)})\n",
        "# Get the indices for the train/val split\n",
        "indices = np.arange(len(custom_dataset))\n",
        "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)  # 80-20 split, adjust as necessary\n",
        "\n",
        "# Create the train and validation datasets using the indices\n",
        "train_data = custom_dataset.select(train_indices)\n",
        "val_data = custom_dataset.select(val_indices)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "kwkMGDt1NUnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data)\n",
        "print(val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6ZgpQEISZbF",
        "outputId": "f50bf564-40bb-4b5c-f33a-2e7f96d4802a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['text', 'label'],\n",
            "    num_rows: 131\n",
            "})\n",
            "Dataset({\n",
            "    features: ['text', 'label'],\n",
            "    num_rows: 33\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkeuMUeDSluk",
        "outputId": "e2dd8c79-8eb5-49a0-d1c8-f4b5c6ad46ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 131\n",
            "Validation samples: 33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389,
          "referenced_widgets": [
            "f79e4ddc8eca48d2b3d586472c679177",
            "52c7415966b84e14b57f75f15eba2608",
            "1d87290b74584ac7ba4980181f066f9f",
            "e9b6a050c09f475b801c3793f7cc48c3",
            "7f7ace609f41494dbb9dd915425c5eec",
            "ce5ab259474a49cfb46a5f24e35cee1e",
            "6f5e95778c7f47719bb6b24aab53262a",
            "da8206fd2f074deab26bcb9e96289a37",
            "af73c30ac45e48a089de8f2765349b30",
            "77149f205f5a4cb98be5b6e02c0d1227",
            "199a979d764e4e25801d57f947be5629",
            "fe53dbb171f64a3a89ed03bc26a38804",
            "a2f0d1a3970a434f94d7e2573dff971b",
            "4b6aea3ee32d41f2a389332c1794f491",
            "5c6f1bbc18bf45c1869bd2997ae1a362",
            "b129659c7c5e490e9ca7cbe08070e481",
            "c35b7ec0fc8f4517b1f6b0b899b6f264",
            "d26a19d2049d476fb1caf58220a9bfea",
            "b9dce4bb757d4b7fab63996fcae26ec4",
            "09781f1c2fe04ffcb6b8f7d01435b5fb",
            "bd99acec3b714607a50e5abafe2fb08a",
            "a221ccf9248a4643840d826ad7c0ecb2",
            "14b4ae0600c840b6898596a85cd00acc",
            "20d578b7c05a46b982b76563fa604739",
            "460b40aabcaa4749929d21a9b315a957",
            "67c8018ae7a246cb98d3147917162c37",
            "b17bc63b903c411a90384fa169854b6a",
            "234d96e89f8f42dc90c66908775c1087",
            "3aeacafd02b54b21895df729ca475d3d",
            "2fd5f41a34ab4baf8e494f303d7d55bb",
            "41f0c603795b4123ad81ed1707831b12",
            "949ceab18afa422e875c5afb3fdaf600",
            "7fad4501519341549c5d4715011dcdf5"
          ]
        },
        "collapsed": true,
        "id": "gRGfHqYk3D_U",
        "outputId": "2e804909-22f6-4806-b44a-517c41449721"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/164 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f79e4ddc8eca48d2b3d586472c679177"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/131 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe53dbb171f64a3a89ed03bc26a38804"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14b4ae0600c840b6898596a85cd00acc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4698374569416046, 'eval_accuracy': 1.0, 'eval_runtime': 1.033, 'eval_samples_per_second': 31.946, 'eval_steps_per_second': 8.713, 'epoch': 1.0}\n",
            "{'eval_loss': 0.09122232347726822, 'eval_accuracy': 1.0, 'eval_runtime': 1.0368, 'eval_samples_per_second': 31.829, 'eval_steps_per_second': 8.681, 'epoch': 2.0}\n",
            "{'eval_loss': 0.006198262330144644, 'eval_accuracy': 1.0, 'eval_runtime': 1.0786, 'eval_samples_per_second': 30.595, 'eval_steps_per_second': 8.344, 'epoch': 3.0}\n",
            "{'loss': 0.3202, 'grad_norm': 0.21348340809345245, 'learning_rate': 1e-05, 'epoch': 3.0303030303030303}\n",
            "{'eval_loss': 0.001715980819426477, 'eval_accuracy': 1.0, 'eval_runtime': 1.0753, 'eval_samples_per_second': 30.689, 'eval_steps_per_second': 8.37, 'epoch': 4.0}\n",
            "{'eval_loss': 0.000926994311157614, 'eval_accuracy': 1.0, 'eval_runtime': 1.0632, 'eval_samples_per_second': 31.039, 'eval_steps_per_second': 8.465, 'epoch': 5.0}\n",
            "{'train_runtime': 145.4071, 'train_samples_per_second': 4.505, 'train_steps_per_second': 1.135, 'train_loss': 0.19522744516531626, 'epoch': 5.0}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=165, training_loss=0.19522744516531626, metrics={'train_runtime': 145.4071, 'train_samples_per_second': 4.505, 'train_steps_per_second': 1.135, 'train_loss': 0.19522744516531626, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,  EvalPrediction, EarlyStoppingCallback\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokenized_dataset = custom_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Define compute_metrics function\n",
        "def compute_metrics(eval_pred: EvalPrediction):\n",
        "  \"\"\"\n",
        "  Calculates and returns a dictionary of metrics.\n",
        "\n",
        "  Args:\n",
        "    eval_pred: An EvalPrediction object containing predictions and labels.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary of metrics, e.g., {'accuracy': 0.85}.\n",
        "  \"\"\"\n",
        "  logits, labels = eval_pred\n",
        "  predictions = np.argmax(logits, axis=-1)\n",
        "  accuracy = np.mean(predictions == labels)\n",
        "  return {'accuracy': accuracy}\n",
        "\n",
        "# Load pre-trained model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Tokenized train and validation datasets\n",
        "tokenized_train = train_data.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_data.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for Trainer compatibility\n",
        "tokenized_train = tokenized_train.with_format(\"torch\")\n",
        "tokenized_val = tokenized_val.with_format(\"torch\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",               # Output directory\n",
        "    evaluation_strategy=\"epoch\",          # Evaluate every epoch\n",
        "    save_strategy=\"epoch\",                # Save model after every epoch\n",
        "    num_train_epochs=5,                   # Number of epochs\n",
        "    per_device_train_batch_size=4,        # Batch size\n",
        "    per_device_eval_batch_size=4,         # Eval batch size\n",
        "    save_total_limit=2,                   # Limit the number of saved models\n",
        "    load_best_model_at_end=True,          # Load the best model based on validation loss\n",
        "    metric_for_best_model=\"eval_loss\",    # Metric to choose the best model\n",
        "    greater_is_better=False,              # Lower loss is better\n",
        "    logging_dir='./logs',                 # Directory to save logs\n",
        "    logging_steps=100,                    # Frequency of logging\n",
        "    weight_decay=0.01,                    # Add weight decay (regularization)\n",
        "    warmup_steps=500,                     # Learning rate warmup\n",
        "    report_to=\"none\",                     # Disable wandb logging\n",
        "    disable_tqdm=True                     # Disable progress bar\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "source": [
        "# Evaluate the model on the validation set using the tokenized validation data\n",
        "results = trainer.evaluate(eval_dataset=tokenized_val)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"Validation Results: {results}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFSwxhlwZb4Z",
        "outputId": "d314ede9-84e6-437b-8f28-cd8030de4054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.000926994311157614, 'eval_accuracy': 1.0, 'eval_runtime': 1.043, 'eval_samples_per_second': 31.639, 'eval_steps_per_second': 8.629, 'epoch': 5.0}\n",
            "Validation Results: {'eval_loss': 0.000926994311157614, 'eval_accuracy': 1.0, 'eval_runtime': 1.043, 'eval_samples_per_second': 31.639, 'eval_steps_per_second': 8.629, 'epoch': 5.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSqZXpSr5uQY"
      },
      "source": [
        "# **8. GPT-4 for Summarization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "b7ofRMz550Hk",
        "outputId": "e2cc8932-ebda-4b1f-95e3-facc70039ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting update\n",
            "  Downloading update-0.0.1-py2.py3-none-any.whl.metadata (696 bytes)\n",
            "Collecting gpt4all\n",
            "  Downloading gpt4all-2.8.2-py3-none-manylinux1_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting style==1.1.0 (from update)\n",
            "  Downloading style-1.1.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gpt4all) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gpt4all) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (2024.12.14)\n",
            "Downloading update-0.0.1-py2.py3-none-any.whl (2.9 kB)\n",
            "Downloading style-1.1.0-py2.py3-none-any.whl (6.4 kB)\n",
            "Downloading gpt4all-2.8.2-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: style, update, gpt4all\n",
            "Successfully installed gpt4all-2.8.2 style-1.1.0 update-0.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install update gpt4all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2KRoJDIQ7Crd",
        "outputId": "59e5da3b-9791-4259-b5c4-1e25f9aa0b91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 4.66G/4.66G [01:10<00:00, 66.5MiB/s]\n",
            "Verifying: 100%|██████████| 4.66G/4.66G [00:08<00:00, 533MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large Language Models (LLMs) are powerful AI models that require significant computational resources to train and use. Running them efficiently on your laptop requires a combination of hardware, software, and optimization techniques. Here's a comprehensive guide to help you get started:\n",
            "\n",
            "**Hardware Requirements:**\n",
            "\n",
            "1. **CPU:** A modern CPU with multiple cores is essential for LLMs. Intel Core i5 or i7 (or AMD equivalent) are good starting points.\n",
            "2. **GPU:** A dedicated graphics card can significantly accelerate computations. NVIDIA GeForce GTX 1060 or higher, or AMD Radeon RX 580 or higher, are recommended.\n",
            "3. **RAM:** Ensure you have at least 16 GB of RAM to handle the model's memory requirements.\n",
            "\n",
            "**Software Requirements:**\n",
            "\n",
            "1. **Python:** LLMs typically rely on Python as their primary programming language. Install Python (version 3.x) and a compatible IDE like PyCharm, Visual Studio Code, or Jupyter Notebook.\n",
            "2. **LLM Framework:** Choose an LLM framework that supports your laptop's hardware:\n",
            "\t* Hugging Face Transformers: A popular library for natural language processing tasks.\n",
            "\t* TensorFlow: A widely used open-source machine learning framework.\n",
            "\t* PyTorch: Another popular deep learning framework.\n",
            "\n",
            "**Optimization Techniques:**\n",
            "\n",
            "1. **Batching:** Divide the input data into smaller batches to reduce memory usage and improve performance.\n",
            "2. **Model Pruning:** Remove unnecessary parameters from the model to reduce computational requirements.\n",
            "3. **Quantization:** Convert floating-point numbers to integers or lower-precision formats (e.g., 8-bit) for faster computations.\n",
            "4. **Distributed Training:** Use distributed training techniques, such as data parallelism or model parallelism, to leverage multiple CPU cores and GPUs.\n",
            "\n",
            "**Additional Tips:**\n",
            "\n",
            "1. **Use a Cloud Service:** If your laptop's hardware is not sufficient, consider using cloud services like Google Colab, AWS SageMaker, or Azure Machine Learning Studio for free or low-cost access to powerful computing resources.\n",
            "2. **Choose the Right Model Size:** Select smaller models (e.g., BERT-base) if you're working with limited computational resources.\n",
            "3. **Monitor Resource Usage:** Keep an eye on your laptop's CPU usage, memory consumption, and GPU utilization using tools like Task Manager or System Monitor to avoid overloading your system.\n",
            "\n",
            "**Example Code:**\n",
            "\n",
            "Here's a simple example of how to run a BERT-based LLM in Python:\n",
            "```python\n",
            "import torch\n",
            "from transformers import BertTokenizer, BertModel\n",
            "\n",
            "# Load the pre-trained model and tokenizer\n",
            "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
            "model = BertModel.from_pretrained('bert-base-uncased')\n",
            "\n",
            "# Define your input text and convert it to a tensor\n",
            "input_text = \"Your input sentence\"\n",
            "inputs = tokenizer.encode(input_text, return_tensors='pt', max_length=512)\n",
            "\n",
            "# Run the model on the input data\n",
            "outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
            "\n",
            "print(outputs)\n",
            "```\n",
            "Remember to adjust your code according to the specific LLM framework and hardware you're using.\n",
            "\n",
            "By following these guidelines, you should be able to run LLMs efficiently on your laptop. Happy experimenting!\n"
          ]
        }
      ],
      "source": [
        "from gpt4all import GPT4All\n",
        "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\") # downloads / loads a 4.66GB LLM\n",
        "with model.chat_session():\n",
        "    print(model.generate(\"How can I run LLMs efficiently on my laptop?\", max_tokens=1024))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmWBffx1Hqhs",
        "outputId": "a9d00591-e917-4fdf-aa5b-a6dae78c1e8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: This appears to be a brief summary of two documents:\n",
            "\n",
            "1. The first document contains some general information or introduction.\n",
            "2. The second document also has some content, but it's not specified what that might be.\n",
            "\n",
            "If you'd like me to help with anything else, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "# Summarize search results\n",
        "def summarize_results(results):\n",
        "    relevant_text = \" \".join([result[0] for result in results])\n",
        "\n",
        "    # Load the GPT4All model here within the function\n",
        "    from gpt4all import GPT4All\n",
        "    gpt4_model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\") # Re-load the model here\n",
        "    with gpt4_model.chat_session(): # Using gpt4_model instead of model\n",
        "        summary = gpt4_model.generate(f\"Summarize this: {relevant_text}\", max_tokens=1024) # Using gpt4_model instead of model\n",
        "    return summary\n",
        "\n",
        "\n",
        "# Example dummy results for testing\n",
        "results = [(\"This is the first document text.\",), (\"This is the second document text.\",)]\n",
        "\n",
        "# Get and summarize search results\n",
        "summary = summarize_results(results)\n",
        "print(\"Summary:\", summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. Error handling for PDF extraction ( extra step )**"
      ],
      "metadata": {
        "id": "TPt7_DywBlJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        with open(pdf_path, \"rb\") as pdf_file:\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "            text = \"\"\n",
        "            for page_num in range(len(pdf_reader.pages)):\n",
        "                page = pdf_reader.pages[page_num]\n",
        "                text += page.extract_text()\n",
        "            if not text:\n",
        "                raise ValueError(\"No text found in the PDF\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        text = \"\"\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "6AEZ5-bcBtNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. Final Workflow**"
      ],
      "metadata": {
        "id": "wuTwUx1hBdT-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8W4fDGTH45J",
        "outputId": "023c0fc4-3a71-4bd2-dad5-7f245e364139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: The article discusses the integration of Artificial Intelligence (AI) in newsrooms and its implications for journalists and media outlets. The authors argue that while AI can bring efficiency, productivity, and new possibilities to journalism, it also raises ethical concerns.\n",
            "\n",
            "Some key points from the article include:\n",
            "\n",
            "1. Media outlets often rely on third-party solutions for AI-powered tools, which can lead to a lack of transparency and control over the models used.\n",
            "2. Journalists' vision is crucial in preserving ethical values when integrating AI solutions into news production processes.\n",
            "3. AI integration has led to changes in professional routines, journalist profiles, and final products, but it will not completely eliminate jobs; rather, certain roles may become redundant.\n",
            "4. The use of generative AI poses questions about algorithmic bias, data privacy, human supervision, and the potential for misinformation.\n",
            "\n",
            "The article also references various studies and reports on the topic, including a survey by Beckett (2019) that found no evidence to suggest that media organizations will operate without journalists in the near future.\n"
          ]
        }
      ],
      "source": [
        "def custom_search_engine(query):\n",
        "    # Search relevant chunks\n",
        "    results = search(query, word2vec_model, document_vectors, chunks)\n",
        "\n",
        "    # Summarize results using GPT-4\n",
        "    summary = summarize_results(results)\n",
        "    return summary\n",
        "\n",
        "# Testing the workflow\n",
        "query = \"Explain the role of AI in media.\"\n",
        "response = custom_search_engine(query)\n",
        "print(\"Response:\", response)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f79e4ddc8eca48d2b3d586472c679177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52c7415966b84e14b57f75f15eba2608",
              "IPY_MODEL_1d87290b74584ac7ba4980181f066f9f",
              "IPY_MODEL_e9b6a050c09f475b801c3793f7cc48c3"
            ],
            "layout": "IPY_MODEL_7f7ace609f41494dbb9dd915425c5eec"
          }
        },
        "52c7415966b84e14b57f75f15eba2608": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce5ab259474a49cfb46a5f24e35cee1e",
            "placeholder": "​",
            "style": "IPY_MODEL_6f5e95778c7f47719bb6b24aab53262a",
            "value": "Map: 100%"
          }
        },
        "1d87290b74584ac7ba4980181f066f9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da8206fd2f074deab26bcb9e96289a37",
            "max": 164,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af73c30ac45e48a089de8f2765349b30",
            "value": 164
          }
        },
        "e9b6a050c09f475b801c3793f7cc48c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77149f205f5a4cb98be5b6e02c0d1227",
            "placeholder": "​",
            "style": "IPY_MODEL_199a979d764e4e25801d57f947be5629",
            "value": " 164/164 [00:00&lt;00:00, 1075.77 examples/s]"
          }
        },
        "7f7ace609f41494dbb9dd915425c5eec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce5ab259474a49cfb46a5f24e35cee1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f5e95778c7f47719bb6b24aab53262a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da8206fd2f074deab26bcb9e96289a37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af73c30ac45e48a089de8f2765349b30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77149f205f5a4cb98be5b6e02c0d1227": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "199a979d764e4e25801d57f947be5629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe53dbb171f64a3a89ed03bc26a38804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2f0d1a3970a434f94d7e2573dff971b",
              "IPY_MODEL_4b6aea3ee32d41f2a389332c1794f491",
              "IPY_MODEL_5c6f1bbc18bf45c1869bd2997ae1a362"
            ],
            "layout": "IPY_MODEL_b129659c7c5e490e9ca7cbe08070e481"
          }
        },
        "a2f0d1a3970a434f94d7e2573dff971b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c35b7ec0fc8f4517b1f6b0b899b6f264",
            "placeholder": "​",
            "style": "IPY_MODEL_d26a19d2049d476fb1caf58220a9bfea",
            "value": "Map: 100%"
          }
        },
        "4b6aea3ee32d41f2a389332c1794f491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9dce4bb757d4b7fab63996fcae26ec4",
            "max": 131,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09781f1c2fe04ffcb6b8f7d01435b5fb",
            "value": 131
          }
        },
        "5c6f1bbc18bf45c1869bd2997ae1a362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd99acec3b714607a50e5abafe2fb08a",
            "placeholder": "​",
            "style": "IPY_MODEL_a221ccf9248a4643840d826ad7c0ecb2",
            "value": " 131/131 [00:00&lt;00:00, 873.26 examples/s]"
          }
        },
        "b129659c7c5e490e9ca7cbe08070e481": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c35b7ec0fc8f4517b1f6b0b899b6f264": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d26a19d2049d476fb1caf58220a9bfea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9dce4bb757d4b7fab63996fcae26ec4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09781f1c2fe04ffcb6b8f7d01435b5fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd99acec3b714607a50e5abafe2fb08a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a221ccf9248a4643840d826ad7c0ecb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14b4ae0600c840b6898596a85cd00acc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20d578b7c05a46b982b76563fa604739",
              "IPY_MODEL_460b40aabcaa4749929d21a9b315a957",
              "IPY_MODEL_67c8018ae7a246cb98d3147917162c37"
            ],
            "layout": "IPY_MODEL_b17bc63b903c411a90384fa169854b6a"
          }
        },
        "20d578b7c05a46b982b76563fa604739": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_234d96e89f8f42dc90c66908775c1087",
            "placeholder": "​",
            "style": "IPY_MODEL_3aeacafd02b54b21895df729ca475d3d",
            "value": "Map: 100%"
          }
        },
        "460b40aabcaa4749929d21a9b315a957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fd5f41a34ab4baf8e494f303d7d55bb",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41f0c603795b4123ad81ed1707831b12",
            "value": 33
          }
        },
        "67c8018ae7a246cb98d3147917162c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_949ceab18afa422e875c5afb3fdaf600",
            "placeholder": "​",
            "style": "IPY_MODEL_7fad4501519341549c5d4715011dcdf5",
            "value": " 33/33 [00:00&lt;00:00, 555.57 examples/s]"
          }
        },
        "b17bc63b903c411a90384fa169854b6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "234d96e89f8f42dc90c66908775c1087": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3aeacafd02b54b21895df729ca475d3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fd5f41a34ab4baf8e494f303d7d55bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41f0c603795b4123ad81ed1707831b12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "949ceab18afa422e875c5afb3fdaf600": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fad4501519341549c5d4715011dcdf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}