{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgnbk+6e2qKhBsN4+idBgF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Namrahh/AI-Projects/blob/main/clean_LLM_searchengine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgJl3qQJSDPc"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload your PDF file\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "hFzPMXaDSOgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "# List of PDF paths (the keys of the uploaded files)\n",
        "pdf_paths = list(uploaded.keys())\n",
        "\n",
        "# Extract text from each PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, \"rb\") as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Dictionary to store extracted text from each PDF\n",
        "pdf_texts = {}\n",
        "\n",
        "# Loop through each uploaded PDF file\n",
        "for pdf_path in pdf_paths:\n",
        "    pdf_texts[pdf_path] = extract_text_from_pdf(pdf_path)\n",
        "    print(f\"Extracted Text from {pdf_path} (sample):\", pdf_texts[pdf_path][:500])  # Print a small sample of the extracted text\n",
        "\n",
        "# Optionally, if you want to combine the text from all PDFs into one large corpus\n",
        "combined_text = \"\\n\".join(pdf_texts.values())\n",
        "\n",
        "# Now you can continue with your further processing using `pdf_texts` or `combined_text`\n"
      ],
      "metadata": {
        "id": "M1EKztW3SQw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate chunks from the PDF text\n",
        "# Concatenate all text into a single string before splitting\n",
        "all_text = \" \".join(pdf_texts.values())\n",
        "\n",
        "def split_text_into_chunks(text, chunk_size=1000):\n",
        "    \"\"\"Splits the text into chunks of a specified size.\n",
        "\n",
        "    Args:\n",
        "        text: The text to split.\n",
        "        chunk_size: The desired size of each chunk. Defaults to 1000.\n",
        "\n",
        "    Returns:\n",
        "        A list of text chunks.\n",
        "    \"\"\"\n",
        "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "chunks = list(split_text_into_chunks(all_text))\n",
        "print(\"Number of Chunks:\", len(chunks))\n",
        "print(\"Sample Chunk:\", chunks[0])"
      ],
      "metadata": {
        "id": "5nUDt0GhSToB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Preprocess the chunks\n",
        "preprocessed_corpus = [simple_preprocess(chunk) for chunk in chunks]\n",
        "print(\"Preprocessed Corpus Sample:\", preprocessed_corpus[:2])  # Print first two processed chunks"
      ],
      "metadata": {
        "id": "U4r9L-aeSZLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=preprocessed_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
        "print(\"Word2Vec Model Trained\")"
      ],
      "metadata": {
        "id": "lDXuiiI1SZ0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Generate document vectors by averaging word embeddings\n",
        "def compute_doc_vector(doc, model):\n",
        "    vectors = [model.wv[word] for word in doc if word in model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
        "\n",
        "# Generate vectors for all chunks\n",
        "document_vectors = [compute_doc_vector(doc, word2vec_model) for doc in preprocessed_corpus]\n",
        "\n",
        "# Search Function\n",
        "def search(query, model, document_vectors, original_chunks):\n",
        "    # Preprocess and vectorize query\n",
        "    query_vector = compute_doc_vector(simple_preprocess(query), model)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarities = cosine_similarity([query_vector], document_vectors)\n",
        "\n",
        "    # Rank results\n",
        "    ranked_indices = np.argsort(similarities[0])[::-1]\n",
        "\n",
        "    # Retrieve results\n",
        "    results = [(original_chunks[i], similarities[0][i]) for i in ranked_indices[:5]]  # Top 5 results\n",
        "    return results\n",
        "\n",
        "# Test the search\n",
        "query = \"What is deep learning?\"\n",
        "results = search(query, word2vec_model, document_vectors, chunks)\n",
        "print(\"Search Results:\")\n",
        "for result in results:\n",
        "    print(result)\n"
      ],
      "metadata": {
        "id": "9Et5gBEpScxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "r7wUeNH8SfpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers"
      ],
      "metadata": {
        "id": "-3mKEZyzSho-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have your dataset in `custom_dataset`\n",
        "# Define custom_dataset first\n",
        "custom_dataset = Dataset.from_dict({\"text\": chunks, \"label\": [0] * len(chunks)})\n",
        "# Get the indices for the train/val split\n",
        "indices = np.arange(len(custom_dataset))\n",
        "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)  # 80-20 split, adjust as necessary\n",
        "\n",
        "# Create the train and validation datasets using the indices\n",
        "train_data = custom_dataset.select(train_indices)\n",
        "val_data = custom_dataset.select(val_indices)"
      ],
      "metadata": {
        "id": "szNrPN31SjyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data)\n",
        "print(val_data)"
      ],
      "metadata": {
        "id": "xXejHL7KSmRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")"
      ],
      "metadata": {
        "id": "4aLGmIn5SoLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,  EvalPrediction, EarlyStoppingCallback\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokenized_dataset = custom_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Define compute_metrics function\n",
        "def compute_metrics(eval_pred: EvalPrediction):\n",
        "  \"\"\"\n",
        "  Calculates and returns a dictionary of metrics.\n",
        "\n",
        "  Args:\n",
        "    eval_pred: An EvalPrediction object containing predictions and labels.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary of metrics, e.g., {'accuracy': 0.85}.\n",
        "  \"\"\"\n",
        "  logits, labels = eval_pred\n",
        "  predictions = np.argmax(logits, axis=-1)\n",
        "  accuracy = np.mean(predictions == labels)\n",
        "  return {'accuracy': accuracy}\n",
        "\n",
        "# Load pre-trained model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Tokenized train and validation datasets\n",
        "tokenized_train = train_data.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_data.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for Trainer compatibility\n",
        "tokenized_train = tokenized_train.with_format(\"torch\")\n",
        "tokenized_val = tokenized_val.with_format(\"torch\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",               # Output directory\n",
        "    evaluation_strategy=\"epoch\",          # Evaluate every epoch\n",
        "    save_strategy=\"epoch\",                # Save model after every epoch\n",
        "    num_train_epochs=5,                   # Number of epochs\n",
        "    per_device_train_batch_size=4,        # Batch size\n",
        "    per_device_eval_batch_size=4,         # Eval batch size\n",
        "    save_total_limit=2,                   # Limit the number of saved models\n",
        "    load_best_model_at_end=True,          # Load the best model based on validation loss\n",
        "    metric_for_best_model=\"eval_loss\",    # Metric to choose the best model\n",
        "    greater_is_better=False,              # Lower loss is better\n",
        "    logging_dir='./logs',                 # Directory to save logs\n",
        "    logging_steps=100,                    # Frequency of logging\n",
        "    weight_decay=0.01,                    # Add weight decay (regularization)\n",
        "    warmup_steps=500,                     # Learning rate warmup\n",
        "    report_to=\"none\",                     # Disable wandb logging\n",
        "    disable_tqdm=True                     # Disable progress bar\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "vHjzzNSASqMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the validation set using the tokenized validation data\n",
        "results = trainer.evaluate(eval_dataset=tokenized_val)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"Validation Results: {results}\")"
      ],
      "metadata": {
        "id": "4JyQXjPcSuD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install update gpt4all"
      ],
      "metadata": {
        "id": "yy3OvBA7SwY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt4all import GPT4All\n",
        "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\") # downloads / loads a 4.66GB LLM\n",
        "with model.chat_session():\n",
        "    print(model.generate(\"How can I run LLMs efficiently on my laptop?\", max_tokens=1024))"
      ],
      "metadata": {
        "id": "YiYRWyyBSzZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize search results\n",
        "def summarize_results(results):\n",
        "    relevant_text = \" \".join([result[0] for result in results])\n",
        "\n",
        "    # Load the GPT4All model here within the function\n",
        "    from gpt4all import GPT4All\n",
        "    gpt4_model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\") # Re-load the model here\n",
        "    with gpt4_model.chat_session(): # Using gpt4_model instead of model\n",
        "        summary = gpt4_model.generate(f\"Summarize this: {relevant_text}\", max_tokens=1024) # Using gpt4_model instead of model\n",
        "    return summary\n",
        "\n",
        "\n",
        "# Example dummy results for testing\n",
        "results = [(\"This is the first document text.\",), (\"This is the second document text.\",)]\n",
        "\n",
        "# Get and summarize search results\n",
        "summary = summarize_results(results)\n",
        "print(\"Summary:\", summary)"
      ],
      "metadata": {
        "id": "Y-r-rf5hS16K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        with open(pdf_path, \"rb\") as pdf_file:\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "            text = \"\"\n",
        "            for page_num in range(len(pdf_reader.pages)):\n",
        "                page = pdf_reader.pages[page_num]\n",
        "                text += page.extract_text()\n",
        "            if not text:\n",
        "                raise ValueError(\"No text found in the PDF\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        text = \"\"\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "MRMU3vfUS4Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_search_engine(query):\n",
        "    # Search relevant chunks\n",
        "    results = search(query, word2vec_model, document_vectors, chunks)\n",
        "\n",
        "    # Summarize results using GPT-4\n",
        "    summary = summarize_results(results)\n",
        "    return summary\n",
        "\n",
        "# Testing the workflow\n",
        "query = \"Explain the role of AI in media.\"\n",
        "response = custom_search_engine(query)\n",
        "print(\"Response:\", response)\n"
      ],
      "metadata": {
        "id": "aSnR4Uj1S6j0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}